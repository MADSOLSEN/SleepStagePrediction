{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training and Evaluation\n",
    "This notebook presents a minimal example of how to setup, train and evaluate the deep learning architecture presented in: \n",
    "*\"A Flexible Deep Learning Architecture for Temporal Sleep Stage Classification using Accelerometry and Photoplethysmography\".* \n",
    "\n",
    "The notebook is organized following the brilliant work from Chambon et al. 2018 $^{1}$.\n",
    "\n",
    "Requirements: \n",
    "1. Install repository.\n",
    "\t- pip install -e\n",
    "2. Install dependencies\n",
    "\t- dependencies\n",
    "3. Prepare signal modalities and annotation in h5 files.\n",
    "\t- minimal_example/to_h5.py\n",
    "\n",
    "###### $^{1}$ \"S. Chambon, V. Thorey, P. J. Arnal, E. Mignot, A. Gramfort, \"A deep learning architecture to detect events in EEG signals during sleep.\" IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP), 2018. [[Paper](https://arxiv.org/abs/1807.05981)|[Github](https://github.com/Dreem-Organization/dosed)]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#print(os.getcwd()) # data path is relative to the current working directory, which is found using: print(os.getcwd()) \n",
    "os.chdir('C:\\\\Users\\\\mads_\\\\OneDrive - Danmarks Tekniske Universitet\\\\Dokumenter\\\\python\\\\MasterAlgorithm')\n",
    "from datasets import get_train_validation_test\n",
    "\n",
    "seed = 2022\n",
    "#print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data path is relative to the current working directory, which is found using: print(os.getcwd()) \n",
    "\n",
    "data_directory = '..\\MasterAlgorithm\\data\\h5'\n",
    "training_set, evaluation_set, test_set = get_train_validation_test(data_directory,\n",
    "                                                    percent_test=40,\n",
    "                                                    percent_validation=20,\n",
    "                                                    seed=seed)\n",
    "\n",
    "# Now we are ready \n",
    "print('training set: {}'.format(len(training_set)))\n",
    "print('evaluation set: {}'.format(len(evaluation_set)))\n",
    "print('test set: {}'.format(len(test_set)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset preparation\n",
    "\n",
    "## Signal preprocessing. \n",
    "- \"h5_path\": string - path within h5 file to locate exact modality. \n",
    "- \"channel_idx\": list of integers - channel indexes to extract from the signal modality. \n",
    "- \"preprocessing\": list of dicts - defining preprocessing steps. A signal modality can have an arbitrary number of pre-processing steps.\n",
    "- \"batch_normalization\": dict - assigning normalization actions to do during training. \n",
    "- \"transformations\": dict - assigning transformation (augmentation) operations to process the signal input with during training. \n",
    "- \"add\": boolean - assinging whether the different signal channels should be added after initial preprocessing (to reduce dimensionality).\n",
    "- \"fs_post\": float - sample frequency of the preprocessed signal. \n",
    "- \"dimensions\": list of integers - dimensions of the preprocessed signal. \n",
    "\n",
    "Our algorithm assumes that both ACC and PPG have a sampling frequency of 32 Hz. If they do not, a resampling step should be added as the first preprocessing operation. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "signals_format = {\n",
    "    'ACC_merge': {\n",
    "        'h5_path': 'acc_signal',\n",
    "        'channel_idx': [0, 1, 2],\n",
    "        'preprocessing': [\n",
    "            {\n",
    "                'type': 'median',\n",
    "                'args': {\n",
    "                    'window_size': 30\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'iqr_normalization_adaptive',\n",
    "                'args': {\n",
    "                    'median_window': 30001,\n",
    "                    'iqr_window': 30001\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'clip_by_iqr',\n",
    "                'args': {\n",
    "                    'threshold': 20\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'cal_psd',\n",
    "                'args': {\n",
    "                    'window': 10 * 32, \n",
    "                    'noverlap': 8 * 32, \n",
    "                    'nfft': int(2 ** np.ceil(np.log2(10 * 32))),\n",
    "                    'f_min': 0,\n",
    "                    'f_max': 12,\n",
    "                    'f_sub': 3\n",
    "                }\n",
    "            }\n",
    "\n",
    "        ],\n",
    "        'batch_normalization': {},\n",
    "        'transformations': {\n",
    "            #'image_translation': {},\n",
    "            #'time_mask': {},\n",
    "            #'freq_mask': {},\n",
    "        },\n",
    "        'add': True,\n",
    "        'fs_post': 1,\n",
    "        'dimensions': [int(2 ** np.ceil(np.log2(10 * 32)) / 32 * (12 - 0)) // 3, 1]\n",
    "    },\n",
    "    'ppg_signal': {\n",
    "        'h5_path': 'ppg_signal',\n",
    "        'channel_idx': [0],\n",
    "        'preprocessing': [\n",
    "            {\n",
    "                'type': 'zscore',\n",
    "                'args': {}\n",
    "            },\n",
    "            {\n",
    "                'type': 'change_PPG_direction',\n",
    "                'args': {}\n",
    "            },\n",
    "            {\n",
    "                'type': 'iqr_normalization_adaptive',\n",
    "                'args': {\n",
    "                    'median_window': 301,\n",
    "                    'iqr_window': 301\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'clip_by_iqr',\n",
    "                'args': {\n",
    "                    'threshold': 20\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'cal_psd',\n",
    "                'args': {\n",
    "                    'window': 10 * 32,\n",
    "                    'noverlap': 8 * 32,\n",
    "                    'nfft': int(2 ** np.ceil(np.log2(10 * 32))),\n",
    "                    'f_min': 0.1,\n",
    "                    'f_max': 4.1,\n",
    "                    'f_sub': 1\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'batch_normalization': {},\n",
    "        'transformations': {\n",
    "            #'image_translation': {},\n",
    "            #'time_mask': {},\n",
    "            #'freq_mask': {},\n",
    "        },\n",
    "        'add': False,\n",
    "        'fs_post': 1,\n",
    "        'dimensions': [int(2 ** np.ceil(np.log2(10 * 32)) / 32 * (4.1 - 0.1)) // 1, 1]\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Event format\n",
    "events are formatted with a name, their h5 relative path, and a probability. This probability is used during batch generation to balance the events. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events = ['wake', 'light', 'deep', 'rem']\n",
    "events_format = [\n",
    "    {\n",
    "        'name': 'wake', \n",
    "        'h5_path': 'wake',\n",
    "        'probability': 1 / len(events)\n",
    "    },\n",
    "    {\n",
    "        'name': 'light', \n",
    "        'h5_path': 'light',\n",
    "        'probability': 1 / len(events)\n",
    "    },\n",
    "    {\n",
    "        'name': 'deep', \n",
    "        'h5_path': 'deep',\n",
    "        'probability': 1 / len(events)\n",
    "    },\n",
    "    {\n",
    "        'name': 'rem', \n",
    "        'h5_path': 'rem',\n",
    "        'probability': 1 / len(events)\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset class\n",
    "The dataset class works as batch generator at training time. I handles preprocessing of the signal modalities, that are loaded from their specified h5 directories. \n",
    "\n",
    "- \"records\": list of strings - list of record filenames.\n",
    "- \"h5_directory\": string - h5 directory of data files.\n",
    "- \"signal_format\": list of dicts - directory and preprocessing steps for each signal modality (see above). \n",
    "- \"window\": interger - temporal window segment size (in seconds). \n",
    "- \"number_of_channels\": interger - number of signal modality inputs. \n",
    "- \"events_format\": list of dicts - format of events to model.  \n",
    "- \"prediction_resolution\": integer - model output resolution (in seconds). \n",
    "- \"overlap\": float - consecutive window segments have this assigned overlap. Not used during balanced sampling. \n",
    "- \"minimum_overlap\": float - when signals are segmented, there is a risk of cutting off events. The minimum overlap is the required duration of an event that is cut off relative to the window size. \n",
    "- \"batch_size\": integer - batch size. \n",
    "- \"mode\": string - \"inference\" or \"training\".\n",
    "- \"cache_data\": boolean - Cache preprocessing using Joblib.  \n",
    "- \"n_jobs\": integer - starts parallel preprocessing. max is number of cores supported by local system.\n",
    "- \"seed\": integer - seed.\n",
    "- \"use_mask\": boolean - whether to apply mask. Mask must be defined as event in h5 file. \n",
    "- \"load_signal_in_RAM\": boolean - whether to load all preprocessed data in RAM during trianing (faster but requires memory)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import DatasetGenerator, BalancedDatasetGenerator\n",
    "\n",
    "dataset_params = {\n",
    "    \"h5_directory\": data_directory, \n",
    "    \"signals_format\": signals_format,\n",
    "    \"window\": 30 * 2 ** 8, \n",
    "    \"number_of_channels\": len(signals_format), \n",
    "    \"events_format\": events_format,\n",
    "    \"prediction_resolution\": 30,\n",
    "    \"overlap\": 0.25,\n",
    "    \"minimum_overlap\": 0.1,\n",
    "    \"batch_size\": 2,\n",
    "    \"cache_data\": True,\n",
    "    \"n_jobs\": 4,\n",
    "    \"use_mask\": True,\n",
    "    \"load_signal_in_RAM\": True\n",
    "}\n",
    "\n",
    "ds_train = DatasetGenerator(records=training_set, mode=\"train\", **dataset_params)\n",
    "ds_evaluate = DatasetGenerator(records=evaluation_set, mode=\"inference\", **dataset_params)\n",
    "ds_test = DatasetGenerator(records=test_set, mode=\"inference\", **dataset_params)\n",
    "\n",
    "signal, event = ds_train.__getitem__(0)\n",
    "print(signal.shape)\n",
    "print(event.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model creation\n",
    "\n",
    "- \"input shape\": list of integers: [T, F, C] - Temporal dimension, Spatial dimension, Channel dimension - inferred from signals_format. \n",
    "- \"num_classes\": integer - number of classes - inferred from events_format\n",
    "- \"num_outputs\": integer - number of model outputs (timesteps) - inferred from the signals_format\n",
    "- \"depth\": integer integer - number of encoder and decoder layers in the network - M.\n",
    "- \"init_filter_num\": integer - number of filters of the first encoder layer. \n",
    "- \"filter_increment_factor\": float - number of filters of layer n = number of filters of layer n-1 * filter_increment_factor. \n",
    "- \"max_pool_size\": tuple of integers - maxpool size used in each layer. \n",
    "- \"kernel size\": tuple of integers - kernel size of filters in each layer. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models import ResUNet\n",
    "\n",
    "# model creation\n",
    "model_params = {\n",
    "    'input_shape': [ds_train.fsTime * ds_train.window, ds_train.nSpace, ds_train.nChannels], \n",
    "    'num_classes': len(events),\n",
    "    'num_outputs': ds_train.window // ds_train.prediction_resolution,\n",
    "    'depth': 9,\n",
    "    'init_filter_num': 16,\n",
    "    'filter_increment_factor': 2 ** (1 / 3),\n",
    "    'max_pool_size': (2, 2),\n",
    "    'kernel_size': (16, 3)\n",
    "}\n",
    "\n",
    "resunet = ResUNet(**model_params)\n",
    "resunet.summary() # print summary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from functions import loss_functions, metrics\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Model "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from functions import metrics\n",
    "# f1_by_class = metrics['f1_by_class'](beta=1, class_idx=0)\n",
    "kappa = metrics['cohens_kappa'](num_classes=len(events))\n",
    "acc = metrics['binary_accuracy']()\n",
    "acc_mu, kappa_mu = [], []\n",
    "\n",
    "for idx in range(ds_test.__len__()): \n",
    "    \n",
    "    sig, tar = ds_test.__getitem__(idx)\n",
    "    pre = resunet.predict(sig)\n",
    "    \n",
    "    acc_mu += [acc(pre, tar)]\n",
    "    kappa_mu += [kappa(pre, tar)]\n",
    "    \n",
    "print('accuracy: ', sum(acc_mu) / len(acc_mu))\n",
    "print('Cohens kappa: ', sum(kappa_mu) / len(kappa_mu))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}