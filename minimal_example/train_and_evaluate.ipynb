{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training and Evaluation\n",
    "This notebook presents a minimal example of how to setup, train and evaluate the deep learning architecture presented in: \n",
    "*\"A Flexible Deep Learning Architecture for Temporal Sleep Stage Classification using Accelerometry and Photoplethysmography\".* \n",
    "\n",
    "The notebook is organized following the brilliant work from Chambon et al. 2018 ^1.\n",
    "\n",
    "Requirements: \n",
    "1. Install repository.\n",
    "\t- pip install -e\n",
    "2. Install dependencies\n",
    "\t- dependencies\n",
    "3. Prepare signal modalities and annotation in h5 files.\n",
    "\t- minimal_example/to_h5.py\n",
    "\n",
    "###### ^2^ {{\"S. Chambon, V. Thorey, P. J. Arnal, E. Mignot, A. Gramfort, \"A deep learning architecture to detect events in EEG signals during sleep.\" IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP), 2018. [[Paper](https://arxiv.org/abs/1807.05981)|[Github](https://github.com/Dreem-Organization/dosed)]\"| fndetail: 1}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from datasets import get_train_validation_test, Dataset, BalancedDataset\n",
    "\n",
    "\n",
    "seed = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 16\nevaluation set: 4\ntest set: 12\n"
     ]
    }
   ],
   "source": [
    "# data path is relative to the current working directory, which is found using: print(os.getcwd()) \n",
    "\n",
    "data_directory = '..\\MasterAlgorithm\\data\\h5'\n",
    "training_set, evaluation_set, test_set = get_train_validation_test(data_directory,\n",
    "                                                    percent_test=40,\n",
    "                                                    percent_validation=20,\n",
    "                                                    seed=seed)\n",
    "\n",
    "# Now we are ready \n",
    "print('training set: {}'.format(len(training_set)))\n",
    "print('evaluation set: {}'.format(len(evaluation_set)))\n",
    "print('test set: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "# ==============================================================================================================================================================\n",
    "\n",
    "## Signal preprocessing. \n",
    "- \"h5_path\": string - path within h5 file to locate exact modality. \n",
    "- \"channel_idx\": list of integers - channel indexes to extract from the signal modality. \n",
    "- \"preprocessing\": list of dicts - defining preprocessing steps. A signal modality can have an arbitrary number of pre-processing steps.\n",
    "- \"batch_normalization\": dict - assigning normalization actions to do during training. \n",
    "- \"transformations\": dict - assigning transformation (augmentation) operations to process the signal input with during training. \n",
    "- \"add\": boolean - assinging whether the different signal channels should be added after initial preprocessing (to reduce dimensionality).\n",
    "- \"fs_post\": float - sample frequency of the preprocessed signal. \n",
    "- \"dimensions\": list of integers - dimensions of the preprocessed signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_format = {\n",
    "    'ACC_merge': {\n",
    "        'h5_path': 'acc_signal',\n",
    "        'channel_idx': [0, 1, 2],\n",
    "        'preprocessing': [\n",
    "            {\n",
    "                'type': 'median',\n",
    "                'args': {\n",
    "                    'window_size': 30\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'iqr_normalization_adaptive',\n",
    "                'args': {\n",
    "                    'median_window': 30001,\n",
    "                    'iqr_window': 30001\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'clip_by_iqr',\n",
    "                'args': {\n",
    "                    'threshold': 20\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'cal_psd',\n",
    "                'args': {\n",
    "                    'window': 10 * 32, \n",
    "                    'noverlap': 8 * 32, \n",
    "                    'nfft': int(2 ** np.ceil(np.log2(10 * 32))),\n",
    "                    'f_min': 0,\n",
    "                    'f_max': 12,\n",
    "                    'f_sub': 3\n",
    "                }\n",
    "            }\n",
    "\n",
    "        ],\n",
    "        'batch_normalization': {},\n",
    "        'transformations': {\n",
    "            #'image_translation': {},\n",
    "            #'time_mask': {},\n",
    "            #'freq_mask': {},\n",
    "        },\n",
    "        'add': True,\n",
    "        'fs_post': 32,\n",
    "        'dimensions': [int(2 ** np.ceil(np.log2(10 * 32)) / 32 * (6 - 0)) // 3, 1]\n",
    "    },\n",
    "    'ppg_signal': {\n",
    "        'h5_path': 'ppg_signal',\n",
    "        'channel_idx': [0],\n",
    "        'preprocessing': [\n",
    "            {\n",
    "                'type': 'zscore',\n",
    "                'args': {}\n",
    "            },\n",
    "            {\n",
    "                'type': 'change_PPG_direction',\n",
    "                'args': {}\n",
    "            },\n",
    "            {\n",
    "                'type': 'iqr_normalization_adaptive',\n",
    "                'args': {\n",
    "                    'median_window': 301,\n",
    "                    'iqr_window': 301\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'clip_by_iqr',\n",
    "                'args': {\n",
    "                    'threshold': 20\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'cal_psd',\n",
    "                'args': {\n",
    "                    'window': 10 * 32,  # 1/21, 1/35\n",
    "                    'noverlap': 8 * 32,  # 1/42, 1/75\n",
    "                    'nfft': int(2 ** np.ceil(np.log2(10 * 32))),\n",
    "                    'f_min': 0.1,\n",
    "                    'f_max': 2.1,\n",
    "                    'f_sub': 1\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'batch_normalization': {},\n",
    "        'transformations': {\n",
    "            #'image_translation': {},\n",
    "            #'time_mask': {},\n",
    "            #'freq_mask': {},\n",
    "        },\n",
    "        'add': False,\n",
    "        'fs_post': 1,\n",
    "        'dimensions': [int(2 ** np.ceil(np.log2(10 * 32)) / 32 * (2.1 - 0.1)) // 1, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "events = ['wake', 'light', 'deep', 'rem']\n",
    "events_format = []\n",
    "for event in events: \n",
    "    events_format += [{\n",
    "        'name': event, \n",
    "        'h5_path': event,\n",
    "        'probability': 1 / (len(events) + 1)\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "The dataset class works as batch generator at training time. I handles preprocessing of the signal modalities, that are loaded from their specified h5 directories. \n",
    "\n",
    "- \"records\": list of strings - list of record filenames.\n",
    "- \"h5_directory\": string - h5 directory of data files.\n",
    "- \"signal_format\": list of dicts - directory and preprocessing steps for each signal modality (see above). \n",
    "- \"window\": interger - temporal window segment size (in seconds). \n",
    "- \"number_of_channels\": interger - number of signal modality inputs. \n",
    "- \"events_format\": list of dicts - format of events to model.  \n",
    "- \"prediction_resolution\": integer - model output resolution (in seconds). \n",
    "- \"overlap\": float - consecutive window segments have this assigned overlap. Not used during balanced sampling. \n",
    "- \"minimum_overlap\": float - when signals are segmented, there is a risk of cutting off events. The minimum overlap is the required duration of an event that is cut off relative to the window size. \n",
    "- \"batch_size\": integer - batch size. \n",
    "- \"mode\": string - \"inference\" or \"training\".\n",
    "- \"cache_data\": boolean - Cache preprocessing using Joblib.  \n",
    "- \"n_jobs\": integer - starts parallel preprocessing. max is number of cores supported by local system.\n",
    "- \"seed\": integer - seed.\n",
    "- \"use_mask\": boolean - whether to apply mask. Mask must be defined as event in h5 file. \n",
    "- \"load_signal_in_RAM\": boolean - whether to load all preprocessed data in RAM during trianing (faster but requires memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 75%|███████▌  | 12/16 [00:53<00:17,  4.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 16/16 [01:47<00:00,  7.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 16/16 [01:47<00:00,  6.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 4/4 [00:00<00:00, 668.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_params = {\n",
    "    \"h5_directory\": data_directory, \n",
    "    \"signals_format\": signals_format,\n",
    "    \"window\": 30 * 2 ** 10, \n",
    "    \"number_of_channels\": len(signals_format), \n",
    "    \"events_format\": events_format,\n",
    "    \"prediction_resolution\": 30,\n",
    "    \"overlap\": 0.25,\n",
    "    \"minimum_overlap\": 0.1,\n",
    "    \"batch_size\": 2,\n",
    "    \"mode\": 'inference',\n",
    "    \"cache_data\": True,\n",
    "    \"n_jobs\": 4,\n",
    "    \"seed\": seed,\n",
    "    \"use_mask\": True,\n",
    "    \"load_signal_in_RAM\": True\n",
    "}\n",
    "\n",
    "ds_train = Dataset(records=training_set, **dataset_params)\n",
    "ds_evaluate = Dataset(records=evaluation_set, **dataset_params)\n",
    "ds_test = Dataset(records=test_set, **dataset_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Display input/output from each preprocessing method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
