{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training and Evaluation\n",
    "This notebook presents a minimal example of how to setup, train and evaluate the deep learning architecture presented in: \n",
    "*\"A Flexible Deep Learning Architecture for Temporal Sleep Stage Classification using Accelerometry and Photoplethysmography\".* \n",
    "\n",
    "The notebook is organized following the brilliant work from Chambon et al. 2018 $^{1}$.\n",
    "\n",
    "Requirements: \n",
    "1. Install repository.\n",
    "\t- pip install -e\n",
    "2. Install dependencies\n",
    "\t- dependencies\n",
    "3. Prepare signal modalities and annotation in h5 files.\n",
    "\t- minimal_example/to_h5.py\n",
    "\n",
    "###### $^{1}$ \"S. Chambon, V. Thorey, P. J. Arnal, E. Mignot, A. Gramfort, \"A deep learning architecture to detect events in EEG signals during sleep.\" IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP), 2018. [[Paper](https://arxiv.org/abs/1807.05981)|[Github](https://github.com/Dreem-Organization/dosed)]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from dataset import get_train_validation_test, DatasetGenerator, BalancedDatasetGenerator\n",
    "\n",
    "\n",
    "seed = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: 16\nevaluation set: 4\ntest set: 12\n"
     ]
    }
   ],
   "source": [
    "# data path is relative to the current working directory, which is found using: print(os.getcwd()) \n",
    "\n",
    "data_directory = '..\\MasterAlgorithm\\data\\h5'\n",
    "training_set, evaluation_set, test_set = get_train_validation_test(data_directory,\n",
    "                                                    percent_test=40,\n",
    "                                                    percent_validation=20,\n",
    "                                                    seed=seed)\n",
    "\n",
    "# Now we are ready \n",
    "print('training set: {}'.format(len(training_set)))\n",
    "print('evaluation set: {}'.format(len(evaluation_set)))\n",
    "print('test set: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "## Signal preprocessing. \n",
    "- \"h5_path\": string - path within h5 file to locate exact modality. \n",
    "- \"channel_idx\": list of integers - channel indexes to extract from the signal modality. \n",
    "- \"preprocessing\": list of dicts - defining preprocessing steps. A signal modality can have an arbitrary number of pre-processing steps.\n",
    "- \"batch_normalization\": dict - assigning normalization actions to do during training. \n",
    "- \"transformations\": dict - assigning transformation (augmentation) operations to process the signal input with during training. \n",
    "- \"add\": boolean - assinging whether the different signal channels should be added after initial preprocessing (to reduce dimensionality).\n",
    "- \"fs_post\": float - sample frequency of the preprocessed signal. \n",
    "- \"dimensions\": list of integers - dimensions of the preprocessed signal. \n",
    "\n",
    "Our algorithm assumes that both ACC and PPG have a sampling frequency of 32 Hz. If they do not, a resampling step should be added as the first preprocessing operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_format = {\n",
    "    'ACC_merge': {\n",
    "        'h5_path': 'acc_signal',\n",
    "        'channel_idx': [0, 1, 2],\n",
    "        'preprocessing': [\n",
    "            {\n",
    "                'type': 'median',\n",
    "                'args': {\n",
    "                    'window_size': 30\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'iqr_normalization_adaptive',\n",
    "                'args': {\n",
    "                    'median_window': 30001,\n",
    "                    'iqr_window': 30001\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'clip_by_iqr',\n",
    "                'args': {\n",
    "                    'threshold': 20\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'cal_psd',\n",
    "                'args': {\n",
    "                    'window': 10 * 32, \n",
    "                    'noverlap': 8 * 32, \n",
    "                    'nfft': int(2 ** np.ceil(np.log2(10 * 32))),\n",
    "                    'f_min': 0,\n",
    "                    'f_max': 12,\n",
    "                    'f_sub': 3\n",
    "                }\n",
    "            }\n",
    "\n",
    "        ],\n",
    "        'batch_normalization': {},\n",
    "        'transformations': {\n",
    "            #'image_translation': {},\n",
    "            #'time_mask': {},\n",
    "            #'freq_mask': {},\n",
    "        },\n",
    "        'add': True,\n",
    "        'fs_post': 1,\n",
    "        'dimensions': [int(2 ** np.ceil(np.log2(10 * 32)) / 32 * (6 - 0)) // 3, 1]\n",
    "    },\n",
    "    'ppg_signal': {\n",
    "        'h5_path': 'ppg_signal',\n",
    "        'channel_idx': [0],\n",
    "        'preprocessing': [\n",
    "            {\n",
    "                'type': 'zscore',\n",
    "                'args': {}\n",
    "            },\n",
    "            {\n",
    "                'type': 'change_PPG_direction',\n",
    "                'args': {}\n",
    "            },\n",
    "            {\n",
    "                'type': 'iqr_normalization_adaptive',\n",
    "                'args': {\n",
    "                    'median_window': 301,\n",
    "                    'iqr_window': 301\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'clip_by_iqr',\n",
    "                'args': {\n",
    "                    'threshold': 20\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'type': 'cal_psd',\n",
    "                'args': {\n",
    "                    'window': 10 * 32,\n",
    "                    'noverlap': 8 * 32,\n",
    "                    'nfft': int(2 ** np.ceil(np.log2(10 * 32))),\n",
    "                    'f_min': 0.1,\n",
    "                    'f_max': 2.1,\n",
    "                    'f_sub': 1\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'batch_normalization': {},\n",
    "        'transformations': {\n",
    "            #'image_translation': {},\n",
    "            #'time_mask': {},\n",
    "            #'freq_mask': {},\n",
    "        },\n",
    "        'add': False,\n",
    "        'fs_post': 1,\n",
    "        'dimensions': [int(2 ** np.ceil(np.log2(10 * 32)) / 32 * (2.1 - 0.1)) // 1, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event format\n",
    "events are formatted with a name, their h5 relative path, and a probability. This probability is used during batch generation to balance the events. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = ['wake', 'light', 'deep', 'rem']\n",
    "events_format = [\n",
    "    {\n",
    "        'name': 'wake', \n",
    "        'h5_path': 'wake',\n",
    "        'probability': 1 / len(events)\n",
    "    },\n",
    "    {\n",
    "        'name': 'light', \n",
    "        'h5_path': 'light',\n",
    "        'probability': 1 / len(events)\n",
    "    },\n",
    "    {\n",
    "        'name': 'deep', \n",
    "        'h5_path': 'deep',\n",
    "        'probability': 1 / len(events)\n",
    "    },\n",
    "    {\n",
    "        'name': 'rem', \n",
    "        'h5_path': 'rem',\n",
    "        'probability': 1 / len(events)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "The dataset class works as batch generator at training time. I handles preprocessing of the signal modalities, that are loaded from their specified h5 directories. \n",
    "\n",
    "- \"records\": list of strings - list of record filenames.\n",
    "- \"h5_directory\": string - h5 directory of data files.\n",
    "- \"signal_format\": list of dicts - directory and preprocessing steps for each signal modality (see above). \n",
    "- \"window\": interger - temporal window segment size (in seconds). \n",
    "- \"number_of_channels\": interger - number of signal modality inputs. \n",
    "- \"events_format\": list of dicts - format of events to model.  \n",
    "- \"prediction_resolution\": integer - model output resolution (in seconds). \n",
    "- \"overlap\": float - consecutive window segments have this assigned overlap. Not used during balanced sampling. \n",
    "- \"minimum_overlap\": float - when signals are segmented, there is a risk of cutting off events. The minimum overlap is the required duration of an event that is cut off relative to the window size. \n",
    "- \"batch_size\": integer - batch size. \n",
    "- \"mode\": string - \"inference\" or \"training\".\n",
    "- \"cache_data\": boolean - Cache preprocessing using Joblib.  \n",
    "- \"n_jobs\": integer - starts parallel preprocessing. max is number of cores supported by local system.\n",
    "- \"seed\": integer - seed.\n",
    "- \"use_mask\": boolean - whether to apply mask. Mask must be defined as event in h5 file. \n",
    "- \"load_signal_in_RAM\": boolean - whether to load all preprocessed data in RAM during trianing (faster but requires memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 50%|█████     | 8/16 [00:54<00:54,  6.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 75%|███████▌  | 12/16 [01:26<00:29,  7.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 16/16 [02:13<00:00,  8.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 16/16 [02:13<00:00,  8.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 4/4 [00:00<00:00, 501.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 67%|██████▋   | 8/12 [00:29<00:14,  3.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 12/12 [01:20<00:00,  7.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r100%|██████████| 12/12 [01:20<00:00,  6.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_params = {\n",
    "    \"h5_directory\": data_directory, \n",
    "    \"signals_format\": signals_format,\n",
    "    \"window\": 30 * 2 ** 10, \n",
    "    \"number_of_channels\": len(signals_format), \n",
    "    \"events_format\": events_format,\n",
    "    \"prediction_resolution\": 30,\n",
    "    \"overlap\": 0.25,\n",
    "    \"minimum_overlap\": 0.1,\n",
    "    \"batch_size\": 2,\n",
    "    \"cache_data\": True,\n",
    "    \"n_jobs\": 4,\n",
    "    \"use_mask\": True,\n",
    "    \"load_signal_in_RAM\": True\n",
    "}\n",
    "\n",
    "ds_train = DatasetGenerator(records=training_set, mode=\"train\", **dataset_params)\n",
    "ds_evaluate = DatasetGenerator(records=evaluation_set, mode=\"inference\", **dataset_params)\n",
    "ds_test = DatasetGenerator(records=test_set, mode=\"inference\", **dataset_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation\n",
    "\n",
    "- \"input shape\": list of integers: [T, F, C] - Temporal dimension, Spatial dimension, Channel dimension - inferred from signals_format. \n",
    "- \"num_classes\": integer - number of classes - inferred from events_format\n",
    "- \"num_outputs\": integer - number of model outputs (timesteps) - inferred from the signals_format\n",
    "- \"depth\": integer integer - number of encoder and decoder layers in the network - M.\n",
    "- \"init_filter_num\": integer - number of filters of the first encoder layer. \n",
    "- \"filter_increment_factor\": float - number of filters of layer n = number of filters of layer n-1 * filter_increment_factor. \n",
    "- \"max_pool_size\": tuple of integers - maxpool size used in each layer. \n",
    "- \"kernel size\": tuple of integers - kernel size of filters in each layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_names, fs, num_features, num_channels = [], [], [], []\n",
    "for signal_format in signals_format.values():\n",
    "    fs.append(signal_format['fs_post'])\n",
    "    num_features.append(signal_format['dimensions'][0])\n",
    "    num_channels.append(signal_format['dimensions'][1])\n",
    "    #signal_names.append(signal_name)\n",
    "\n",
    "assert(len(set(fs)) == 1) # assert all output sample frequencies are identical.\n",
    "assert(len(set(num_features))) # assert all feature dimensions are identical.\n",
    "fs = fs[0]\n",
    "num_features = num_features[0]\n",
    "num_channels = sum(num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ResUNet\n",
    "\n",
    "# model creation\n",
    "model_params = {\n",
    "    'input_shape': [fs * dataset_params['window'], num_features, num_channels], \n",
    "    'num_classes': len(events),\n",
    "    'num_outputs': dataset_params['window'] // dataset_params['prediction_resolution'],\n",
    "    'depth': 15,\n",
    "    'init_filter_num': 16,\n",
    "    'filter_increment_factor': 2 ** (1 / 3),\n",
    "    'max_pool_size': (2, 2),\n",
    "    'kernel_size': (16, 3)\n",
    "}\n",
    "\n",
    "resunet = ResUNet(**model_params)\n",
    "#resunet.summary() # print summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from functions import loss_functions, metrics\n",
    "\n",
    "\n",
    "resunet.compile(loss=loss_functions['weighted_loss'](**{'alpha': 0.35, 'gamma': 2}),\n",
    "                run_eagerly=False,\n",
    "                optimizer=Adam(learning_rate=1e-4, epsilon=1e-8),\n",
    "                metrics=[metrics['cohens_kappa'](num_classes=len(events))])\n",
    "\n",
    "history = resunet.fit(ds_train,\n",
    "                      epochs=40,\n",
    "                      verbose=1,\n",
    "                      initial_epoch=0,\n",
    "                      validation_data=ds_evaluate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
